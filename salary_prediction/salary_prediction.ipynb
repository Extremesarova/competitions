{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to predict salary based on the different features: \n",
    "* Text features:\n",
    "    * Vacancy Title \n",
    "    * Vacancy Description\n",
    "* Categorical features:\n",
    "    * Category\n",
    "    * Company\n",
    "    * Location\n",
    "    * Contract Type\n",
    "    * Contract Time\n",
    "\n",
    "We are going to be using Deep Learning approach to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "from loguru import logger\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy.stats import beta, shapiro\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will use MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244768, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data.drop([\"LocationRaw\", \"SalaryRaw\", \"SourceName\", \"Id\"], axis=1, inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\n",
    "    \"Category\",\n",
    "    \"Company\",\n",
    "    \"LocationNormalized\",\n",
    "    \"ContractType\",\n",
    "    \"ContractTime\",\n",
    "]\n",
    "target_column = \"SalaryNormalized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30390</th>\n",
       "      <td>Business Account Manager</td>\n",
       "      <td>Business Account Manager  Plumbing Heating Pro...</td>\n",
       "      <td>South West London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>BMS Sales Specialists</td>\n",
       "      <td>Sales Jobs</td>\n",
       "      <td>23500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108709</th>\n",
       "      <td>Staff Nurse RGN/RMN  Newtown Abbey  Nights  Ne...</td>\n",
       "      <td>Job Title; Staff Nurse RGN/RMN – NightsLocatio...</td>\n",
       "      <td>Newtownabbey</td>\n",
       "      <td>full_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Regional Recruitment Services</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>23040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13924</th>\n",
       "      <td>Community Fundraising Manager</td>\n",
       "      <td>This dynamic international development charity...</td>\n",
       "      <td>London</td>\n",
       "      <td>full_time</td>\n",
       "      <td>permanent</td>\n",
       "      <td>TPP Not for Profit</td>\n",
       "      <td>Charity &amp; Voluntary Jobs</td>\n",
       "      <td>34850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154606</th>\n",
       "      <td>Conveyancing Executive</td>\n",
       "      <td>Large regional law firm require a residential ...</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>full_time</td>\n",
       "      <td>permanent</td>\n",
       "      <td>PTP Consulting</td>\n",
       "      <td>Legal Jobs</td>\n",
       "      <td>22500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172891</th>\n",
       "      <td>Investment Compliance Analyst</td>\n",
       "      <td>This individual will work closely with the bus...</td>\n",
       "      <td>South East London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Michael Page Financial Services</td>\n",
       "      <td>Accounting &amp; Finance Jobs</td>\n",
       "      <td>55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164389</th>\n",
       "      <td>SAP Project Manager</td>\n",
       "      <td>SAP Project Manager My client is a management ...</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109469</th>\n",
       "      <td>Sheet Metal Worker</td>\n",
       "      <td>SHEET METAL WORKER ABERDEEN Sheet Metal Worker...</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>full_time</td>\n",
       "      <td>contract</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>24960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214859</th>\n",
       "      <td>Domiciliary Care Trainer</td>\n",
       "      <td>A dedicated and experienced Domiciliary Care T...</td>\n",
       "      <td>Oxford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Vivico Recruitment</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>22000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225029</th>\n",
       "      <td>Kitchen Manager  Branded Restaurant</td>\n",
       "      <td>Kitchen Manager An exciting opportunity has ar...</td>\n",
       "      <td>Luton</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Renard Resources</td>\n",
       "      <td>Hospitality &amp; Catering Jobs</td>\n",
       "      <td>22500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149935</th>\n",
       "      <td>Computational Biologist</td>\n",
       "      <td>Computational Biologist ****K (depending on ex...</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Escape</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>35000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "30390                            Business Account Manager   \n",
       "108709  Staff Nurse RGN/RMN  Newtown Abbey  Nights  Ne...   \n",
       "13924                       Community Fundraising Manager   \n",
       "154606                             Conveyancing Executive   \n",
       "172891                      Investment Compliance Analyst   \n",
       "164389                                SAP Project Manager   \n",
       "109469                                 Sheet Metal Worker   \n",
       "214859                           Domiciliary Care Trainer   \n",
       "225029                Kitchen Manager  Branded Restaurant   \n",
       "149935                            Computational Biologist   \n",
       "\n",
       "                                          FullDescription LocationNormalized  \\\n",
       "30390   Business Account Manager  Plumbing Heating Pro...  South West London   \n",
       "108709  Job Title; Staff Nurse RGN/RMN – NightsLocatio...       Newtownabbey   \n",
       "13924   This dynamic international development charity...             London   \n",
       "154606  Large regional law firm require a residential ...          Hampshire   \n",
       "172891  This individual will work closely with the bus...  South East London   \n",
       "164389  SAP Project Manager My client is a management ...             London   \n",
       "109469  SHEET METAL WORKER ABERDEEN Sheet Metal Worker...           Aberdeen   \n",
       "214859  A dedicated and experienced Domiciliary Care T...             Oxford   \n",
       "225029  Kitchen Manager An exciting opportunity has ar...              Luton   \n",
       "149935  Computational Biologist ****K (depending on ex...                 UK   \n",
       "\n",
       "       ContractType ContractTime                          Company  \\\n",
       "30390           NaN    permanent            BMS Sales Specialists   \n",
       "108709    full_time          NaN    Regional Recruitment Services   \n",
       "13924     full_time    permanent               TPP Not for Profit   \n",
       "154606    full_time    permanent                   PTP Consulting   \n",
       "172891          NaN    permanent  Michael Page Financial Services   \n",
       "164389          NaN    permanent                              NaN   \n",
       "109469    full_time     contract                              NaN   \n",
       "214859          NaN    permanent               Vivico Recruitment   \n",
       "225029          NaN          NaN                 Renard Resources   \n",
       "149935          NaN    permanent                           Escape   \n",
       "\n",
       "                           Category  SalaryNormalized  \n",
       "30390                    Sales Jobs             23500  \n",
       "108709    Healthcare & Nursing Jobs             23040  \n",
       "13924      Charity & Voluntary Jobs             34850  \n",
       "154606                   Legal Jobs             22500  \n",
       "172891    Accounting & Finance Jobs             55000  \n",
       "164389                      IT Jobs             75000  \n",
       "109469             Engineering Jobs             24960  \n",
       "214859    Healthcare & Nursing Jobs             22000  \n",
       "225029  Hospitality & Catering Jobs             22500  \n",
       "149935    Healthcare & Nursing Jobs             35000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                      1\n",
       "FullDescription            0\n",
       "LocationNormalized         0\n",
       "ContractType          179326\n",
       "ContractTime           63905\n",
       "Company                32430\n",
       "Category                   0\n",
       "SalaryNormalized           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just fill all the `NA` values with `NaN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195814, 8), (48954, 8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=SEED)\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_train.shape, data_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Close Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195814 entries, 0 to 195813\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   Title               195814 non-null  object\n",
      " 1   FullDescription     195814 non-null  object\n",
      " 2   LocationNormalized  195814 non-null  object\n",
      " 3   ContractType        195814 non-null  object\n",
      " 4   ContractTime        195814 non-null  object\n",
      " 5   Company             195814 non-null  object\n",
      " 6   Category            195814 non-null  object\n",
      " 7   SalaryNormalized    195814 non-null  int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lognormal distribution is suitable for describing salaries, price of securities, urban population, number of comments on articles on the internet, etc.  \n",
    "So, we will transform salary using `log1p` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log1pSalary = np.log1p(data_train[\"SalaryNormalized\"]).astype(\"float32\")\n",
    "\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log1pSalary, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryblo\\Documents\\projects\\yandex_nlp_course\\venv\\lib\\site-packages\\scipy\\stats\\_morestats.py:1761: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapiroResult(statistic=0.9969059824943542, pvalue=2.802596928649634e-45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro(log1pSalary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is really lognormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Category' column contains 29 unique values\n",
      "'Company' column contains 18857 unique values\n",
      "'LocationNormalized' column contains 2543 unique values\n",
      "'ContractType' column contains 3 unique values\n",
      "'ContractTime' column contains 3 unique values\n"
     ]
    }
   ],
   "source": [
    "for cat_column in categorical_columns:\n",
    "    print(f\"'{cat_column}' column contains {data_train[cat_column].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IT Jobs                             30847\n",
       "Engineering Jobs                    20159\n",
       "Accounting & Finance Jobs           17434\n",
       "Healthcare & Nursing Jobs           16916\n",
       "Sales Jobs                          13849\n",
       "Other/General Jobs                  13567\n",
       "Teaching Jobs                       10137\n",
       "Hospitality & Catering Jobs          9066\n",
       "Trade & Construction Jobs            7088\n",
       "PR, Advertising & Marketing Jobs     7070\n",
       "HR & Recruitment Jobs                6190\n",
       "Admin Jobs                           6112\n",
       "Retail Jobs                          5201\n",
       "Customer Services Jobs               4794\n",
       "Legal Jobs                           3145\n",
       "Manufacturing Jobs                   3027\n",
       "Logistics & Warehouse Jobs           2912\n",
       "Social work Jobs                     2743\n",
       "Consultancy Jobs                     2591\n",
       "Travel Jobs                          2508\n",
       "Scientific & QA Jobs                 2010\n",
       "Charity & Voluntary Jobs             1867\n",
       "Energy, Oil & Gas Jobs               1821\n",
       "Creative & Design Jobs               1285\n",
       "Maintenance Jobs                     1235\n",
       "Graduate Jobs                        1067\n",
       "Property Jobs                         826\n",
       "Domestic help & Cleaning Jobs         227\n",
       "Part time Jobs                        120\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Company column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                                      25884\n",
       "UKStaffsearch                                             4015\n",
       "CVbrowser                                                 2387\n",
       "London4Jobs                                               1859\n",
       "Hays                                                      1424\n",
       "                                                         ...  \n",
       "Oxford University Hospitals NHS Trust                        1\n",
       "Aspirations Care                                             1\n",
       "MANPOWER UK LTD                                              1\n",
       "Zolv.com Ltd                                                 1\n",
       "The Facial Surgery Research Foundation   Saving Faces        1\n",
       "Name: Company, Length: 18857, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"Company\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LocationNormalized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UK                   32767\n",
       "London               24483\n",
       "South East London     9411\n",
       "The City              5337\n",
       "Manchester            2859\n",
       "                     ...  \n",
       "Kilbarchan               1\n",
       "Brompton                 1\n",
       "Kennoway                 1\n",
       "Kirkby Stephen           1\n",
       "Grimethorpe              1\n",
       "Name: LocationNormalized, Length: 2543, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"LocationNormalized\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ContractType column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN          143245\n",
       "full_time     46208\n",
       "part_time      6361\n",
       "Name: ContractType, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"ContractType\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ContractTime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permanent    121042\n",
       "NaN           51332\n",
       "contract      23440\n",
       "Name: ContractTime, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"ContractTime\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using `TweetTokenizer`, because I like how it handles apostrophes (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "# tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "assert tokenizer.tokenize(\"I've been doing this for a few years!\") == [\n",
    "    \"I've\",\n",
    "    \"been\",\n",
    "    \"doing\",\n",
    "    \"this\",\n",
    "    \"for\",\n",
    "    \"a\",\n",
    "    \"few\",\n",
    "    \"years\",\n",
    "    \"!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor class takes as input:\n",
    "* `data` - dataframe with our dataset \n",
    "* `tokenizer` - to split sentences into tokens\n",
    "* `min_count` - minimum count for building vocabulary  \n",
    "* `max_len_title` - maximum number of words for the title\n",
    "* `max_len_description` - maximum number of words for the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessDataset:\n",
    "    TEXT_COLS = [\"Title\", \"FullDescription\"]\n",
    "    CAT_COLS = [\n",
    "        \"Category\",\n",
    "        \"Company\",\n",
    "        \"LocationNormalized\",\n",
    "        \"ContractType\",\n",
    "        \"ContractTime\",\n",
    "    ]\n",
    "    TARGET_COL = \"SalaryNormalized\"\n",
    "    NEW_TARGET_COL = \"Log1pSalary\"\n",
    "    UNK, PAD = \"UNK\", \"PAD\"\n",
    "    SEED = 42\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        min_count=10,\n",
    "        max_len_title=None,\n",
    "        max_len_description=None,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "        self.data_train, self.data_val = self._split_data()\n",
    "\n",
    "        self.data_train = self.preprocess_data(self.data_train, train=True)\n",
    "        self.data_val = self.preprocess_data(self.data_val)\n",
    "\n",
    "        self.categorical_vectorizer = self._preprocess_categorical()\n",
    "        self.vocab = self._build_vocab(min_count)\n",
    "        self.token_to_id = self._build_token_to_id_dict()\n",
    "        self.UNK_IX, self.PAD_IX = map(self.token_to_id.get, [self.UNK, self.PAD])\n",
    "        self.max_len_description = max_len_description\n",
    "        self.max_len_title = max_len_title\n",
    "\n",
    "    def preprocess_data(self, data, train=False):\n",
    "        logger.info(f\"Preprocessing {'Train' if train else 'Val'} data\")\n",
    "        data = self._preprocess_texts(data)\n",
    "        data = self._preprocess_target(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _split_data(self, test_size=0.2):\n",
    "        \"\"\"Split dataset into train and validation sets with specified `test_size` and `random_state\"\"\"\n",
    "        \n",
    "        logger.info(\"Splitting data to train and val sets\")\n",
    "\n",
    "        data_train, data_val = train_test_split(self.data, test_size=test_size, random_state=self.SEED)\n",
    "        data_train.reset_index(drop=True, inplace=True)\n",
    "        data_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return data_train, data_val\n",
    "\n",
    "    def _preprocess_texts(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Tokenize text columns\"\"\"\n",
    "\n",
    "        for text_col in self.TEXT_COLS:\n",
    "            tqdm.pandas(desc=f\"Tokenizing {text_col}\")\n",
    "            tokenized_texts = data[text_col].progress_apply(self._tokenize)\n",
    "            data[text_col] = tokenized_texts.values\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_target(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Make `log1p` transformation for the target column\"\"\"\n",
    "\n",
    "        logger.info(\"Preprocessing Target column\")\n",
    "\n",
    "        data[self.NEW_TARGET_COL] = np.log1p(data[self.TARGET_COL]).astype(\"float32\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _preprocess_categorical(self, most_common=1000):\n",
    "        \"\"\"Preprocess categorical columns (only use top 1000 companies)\"\"\"\n",
    "\n",
    "        logger.info(\"Preprocessing Categorical columns\")\n",
    "\n",
    "        tqdm.pandas(desc=f\"Preprocessing Company column\")\n",
    "        # we only consider top-1k most frequent companies to minimize memory usage\n",
    "        top_companies, top_counts = zip(*Counter(self.data_train[\"Company\"]).most_common(most_common))\n",
    "        recognized_companies = set(top_companies)\n",
    "        self.data_train[\"Company\"] = self.data_train[\"Company\"].progress_apply(\n",
    "            lambda comp: comp if comp in recognized_companies else \"Other\"\n",
    "        )\n",
    "\n",
    "        tqdm.pandas(desc=f\"Vectorizing categorical columns\")\n",
    "        categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "        categorical_vectorizer.fit(self.data_train[self.CAT_COLS].progress_apply(dict, axis=1))\n",
    "\n",
    "        return categorical_vectorizer\n",
    "\n",
    "    def _build_vocab(self, min_count):\n",
    "        \"\"\"Build vocabulary for the text columns considering `min_count`\"\"\"\n",
    "\n",
    "        token_counts = Counter()\n",
    "\n",
    "        # Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
    "        for str_ in tqdm(self.data_train[text_columns].values.flatten(), desc=\"Building vocab\"):\n",
    "            token_counts.update(str(str_).split())\n",
    "\n",
    "        logger.debug(f\"Total unique tokens: {len(token_counts)}\")\n",
    "        logger.debug(\"\\n\".join(map(str, token_counts.most_common(n=5))))\n",
    "        logger.debug(\"\\n\".join(map(str, token_counts.most_common(n=5))))\n",
    "        logger.debug(\"...\")\n",
    "        logger.debug(\"\\n\".join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "        # tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "        tokens = sorted(t for t, c in token_counts.items() if c >= min_count)\n",
    "\n",
    "        # Add a special tokens for unknown and empty words\n",
    "        tokens = [self.UNK, self.PAD] + tokens\n",
    "\n",
    "        logger.debug(f\"Vocabulary size: {len(tokens)}\")\n",
    "        assert type(tokens) == list\n",
    "        assert len(tokens) in range(30000, 35000)\n",
    "        assert \"me\" in tokens\n",
    "        assert self.UNK in tokens\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _build_token_to_id_dict(self):\n",
    "        \"\"\"Build token-to-id dict\"\"\"\n",
    "\n",
    "        token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "\n",
    "        assert isinstance(token_to_id, dict)\n",
    "        assert len(token_to_id) == len(self.vocab)\n",
    "        for token in self.vocab:\n",
    "            assert self.vocab[token_to_id[token]] == token\n",
    "\n",
    "        return token_to_id\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return \" \".join(self.tokenizer.tokenize(str(text).lower()))\n",
    "\n",
    "    def _as_matrix(self, sequences, max_len=None):\n",
    "        \"\"\"Convert a list of tokens into a matrix with padding\"\"\"\n",
    "\n",
    "        if isinstance(sequences[0], str):\n",
    "            sequences = list(map(str.split, sequences))\n",
    "\n",
    "        max_len = min(max(map(len, sequences)), max_len or float(\"inf\"))\n",
    "\n",
    "        matrix = np.full((len(sequences), max_len), np.int32(self.PAD_IX))\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            row_ix = [self.token_to_id.get(word, self.UNK_IX) for word in seq[:max_len]]\n",
    "            matrix[i, : len(row_ix)] = row_ix\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:34:25.267 | INFO     | __main__:_split_data:47 - Splitting data to train and val sets\n",
      "2022-05-19 14:34:25.310 | INFO     | __main__:preprocess_data:39 - Preprocessing Train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c340f90bfe545cc888af345b3c5ca71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Title:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c680145966044ec58580d39889ded455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing FullDescription:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:38:07.856 | INFO     | __main__:_preprocess_target:68 - Preprocessing Target column\n",
      "2022-05-19 14:38:07.856 | INFO     | __main__:preprocess_data:39 - Preprocessing Val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1d4b7b664c4438a4fe8481dc0a0c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Title:   0%|          | 0/48954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408af679daec468497cd6980e8c0fe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing FullDescription:   0%|          | 0/48954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:39:04.257 | INFO     | __main__:_preprocess_target:68 - Preprocessing Target column\n",
      "2022-05-19 14:39:04.265 | INFO     | __main__:_preprocess_categorical:77 - Preprocessing Categorical columns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01544ab427c743a6afbbddd229436074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Company column:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21098f820ee44ec8ed77c0b460eaa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Vectorizing categorical columns:   0%|          | 0/195814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18b5c7a79a94e89afa3aaef8943151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocab:   0%|          | 0/391628 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:39:15.765 | DEBUG    | __main__:_build_vocab:102 - Total unique tokens: 201108\n",
      "2022-05-19 14:39:15.781 | DEBUG    | __main__:_build_vocab:103 - ('and', 2123081)\n",
      "(',', 1871868)\n",
      "('.', 1758719)\n",
      "('*', 1674219)\n",
      "('the', 1659371)\n",
      "2022-05-19 14:39:15.797 | DEBUG    | __main__:_build_vocab:104 - ('and', 2123081)\n",
      "(',', 1871868)\n",
      "('.', 1758719)\n",
      "('*', 1674219)\n",
      "('the', 1659371)\n",
      "2022-05-19 14:39:15.797 | DEBUG    | __main__:_build_vocab:105 - ...\n",
      "2022-05-19 14:39:15.837 | DEBUG    | __main__:_build_vocab:106 - ('pate', 1)\n",
      "('nurseportadown', 1)\n",
      "('www.salestarget.co.uk/jobseeking/salesexecutivesx2northwestlondonesp***_job***', 1)\n",
      "2022-05-19 14:39:15.853 | DEBUG    | __main__:_build_vocab:114 - Vocabulary size: 31171\n"
     ]
    }
   ],
   "source": [
    "preprocessor = PreprocessDataset(data, tokenizer, max_len_title=10, max_len_description=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloaders & Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two Transforms classes: \n",
    "* `ApplyWordDropout` for applaying word dropout\n",
    "* `ToTensor` for converting our data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyWordDropout:\n",
    "    def __init__(self, replace_with, pad_ix, word_dropout=0.0):\n",
    "        self.keep_prop = 1.0 - word_dropout\n",
    "        self.replace_with = replace_with\n",
    "        self.pad_ix = pad_ix\n",
    "\n",
    "    def apply_word_dropout(self, matrix):\n",
    "        dropout_mask = np.random.choice(2, np.shape(matrix), p=[self.keep_prop, 1 - self.keep_prop])\n",
    "        dropout_mask &= matrix != self.pad_ix\n",
    "\n",
    "        return np.choose(dropout_mask, [matrix, np.full_like(matrix, self.replace_with)])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return self.apply_word_dropout(sample)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, device=DEVICE):\n",
    "        sample_tensors = dict()\n",
    "        for key, arr in sample.items():\n",
    "            if key in [\"FullDescription\", \"Title\"]:\n",
    "                sample_tensors[key] = torch.tensor(arr, dtype=torch.long, device=device)\n",
    "            else:\n",
    "                sample_tensors[key] = torch.tensor(arr, device=device)\n",
    "        return sample_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard dataset class for PyTorch, which have methods to get the length of the dataset and to get the item for given id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VacancyDataset:\n",
    "    def __init__(self, preprocessor, train=True, transform=None, word_dropout=0):\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "        self.data = self.preprocessor.data_train if train else preprocessor.data_val\n",
    "\n",
    "        self.transform = transform\n",
    "        self.word_dropout = ApplyWordDropout(\n",
    "            replace_with=self.preprocessor.UNK_IX,\n",
    "            pad_ix=self.preprocessor.PAD_IX,\n",
    "            word_dropout=word_dropout,\n",
    "        )\n",
    "\n",
    "        self.title = self.preprocessor._as_matrix(self.data[\"Title\"].values, self.preprocessor.max_len_title)\n",
    "        self.description = self.preprocessor._as_matrix(\n",
    "            self.data[\"FullDescription\"].values,\n",
    "            self.preprocessor.max_len_description,\n",
    "        )\n",
    "\n",
    "        if word_dropout != 0:\n",
    "            self.description = self.word_dropout(self.description)\n",
    "\n",
    "        self.categorical = self.preprocessor.categorical_vectorizer.transform(\n",
    "            self.data[self.preprocessor.CAT_COLS].apply(dict, axis=1)\n",
    "        )\n",
    "\n",
    "        self.target = self.data[self.preprocessor.NEW_TARGET_COL]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        sample[\"title_text\"] = self.title[idx]\n",
    "        sample[\"description_text\"] = self.description[idx]\n",
    "        sample[\"categorical\"] = self.categorical[idx]\n",
    "        sample[\"target\"] = self.target[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = VacancyDataset(preprocessor, train=True, transform=ToTensor())\n",
    "test_data = VacancyDataset(preprocessor, train=False, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at our data we've prepared for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([24816, 26825, 30259, 29588,   819, 24816, 26825, 30259, 14759, 29588,\n",
       "         11235,   819, 23396, 24881, 13932,     0,    14, 25722,    14, 11235,\n",
       "         23397, 30147, 15941,  8200,    14, 20844,  8286,    14,  1106,  3973,\n",
       "         14235,  1927,     0, 21736,    15, 27600, 13699,  4640, 30041, 12710,\n",
       "          9263,  1883, 19240,    12,    12,    12,    16,    12,    12,    12,\n",
       "         30147,  2591, 15954,   819, 31005, 10309, 30158, 27696, 10848,    14,\n",
       "         21486,  2591,   819, 24816, 16073,    15, 27600, 24816, 26825, 30259,\n",
       "         30041, 12710, 27916,  3220,   898, 27916,  6152,  1927, 13886,  4739,\n",
       "         20988,    14,  2389, 29809,  2389,  2328, 14063,  7402, 19941,  1927,\n",
       "          9527, 23397, 27916, 27125,  1883,  1140, 23897, 13932,  1669,  2467],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'categorical': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'),\n",
       " 'target': tensor(9.7115, device='cuda:0')}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that everything is working fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([[ 9176, 22774,  6487, 31140, 16942,     1,     1,     1,     1,     1],\n",
       "         [17396, 11009,    16, 17394,  1557, 26825,  9831,  1927, 29176,     1],\n",
       "         [13223, 16910, 15888,    16, 17642, 29630,     1,     1,     1,     1],\n",
       "         [16843,  9601,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 9311, 10956,  1927, 24671, 10848, 24233, 16910,     1,     1,     1],\n",
       "         [12903,  7981,  1927, 15268,  1905,     1,     1,     1,     1,     1],\n",
       "         [ 2126, 26825,  9601, 22550,    14, 26041,    14, 28795,  7465, 29604],\n",
       "         [22774,  6487,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [21750,  9601,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [20984,  9601,    14,  6474, 11451,     1,     1,     1,     1,     1],\n",
       "         [17396,  7961,     0, 19703,  6384,    16,  4488,     1,     1,     1],\n",
       "         [24816,  1016, 16910,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24233, 16910,     7, 17418,     9,     1,     1,     1,     1,     1],\n",
       "         [ 8744,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 5663, 24896,  6682,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 9860, 21858, 18063, 19428,     1,     1,     1,     1,     1,     1],\n",
       "         [10209,  5326, 13013, 21814, 23476,     1,     1,     1,     1,     1],\n",
       "         [21764, 24896,  1276,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 6802,  8722,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [20093, 27916, 12785, 19403, 21858,  7801,  5528, 19403, 16464,     1],\n",
       "         [21364, 27281, 23328, 27916, 26202,  2396,     1,     1,     1,     1],\n",
       "         [24806, 27288,  2529,  2416,    16,  2470,     1,     1,     1,     1],\n",
       "         [11064, 16910,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 5663,  1370,    16, 24233,     0,     1,     1,     1,     1,     1],\n",
       "         [22985, 16910, 10432,  4739, 24469, 16803,     1,     1,     1,     1],\n",
       "         [25786,  5326, 11767, 22080,     1,     1,     1,     1,     1,     1],\n",
       "         [21093,    16, 24816, 21093,  1905,     1,     1,     1,     1,     1],\n",
       "         [14830,  1905,     7, 24881,  7801, 15903,     9,     1,     1,     1],\n",
       "         [ 6635,  2529, 15268,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [28248, 20982,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24816,  7967,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [15941,     5,  8058,  6487,     1,     1,     1,     1,     1,     1],\n",
       "         [24603,    16, 27117,    16, 24952,     1,     1,     1,     1,     1],\n",
       "         [17732, 16073,  7465, 21968,     0,     1,     1,     1,     1,     1],\n",
       "         [ 3023,   618, 19347, 27636,  4739, 19403, 27600,  9281,   819,  8967],\n",
       "         [27288,  2529,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [16901,  1024,  2785,  7591, 12331,    14, 12785, 19423,     1,     1],\n",
       "         [18722,  9601,  5003,    16, 19745, 15087, 15090,     0,     1,     1],\n",
       "         [24816, 23949, 15904,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [11214, 27369,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 5326,  7579, 20288, 13013,  9534, 11767, 22080, 20074,     1,     1],\n",
       "         [ 3496, 26825, 16910,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [13402,  1370,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [28163, 22774,  6487,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [14830,  2717,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24233, 10209, 26965,    14,     0,     1,     1,     1,     1,     1],\n",
       "         [ 9401,  4468,    16,  4468,    13,    13, 12781,    14,  8925,    14],\n",
       "         [ 9614, 27281, 24637, 24482,  3606, 29883, 17723, 20640, 21236,     1],\n",
       "         [24842, 10209,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [24331,  3483,  8052, 24110,     7,    12,    12,    12,    12,    12],\n",
       "         [ 8789,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [23657,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [21858, 16910,     7,  2893, 20083,     9,     1,     1,     1,     1],\n",
       "         [12182,  9294,  6602,  9601,  7929,     1,     1,     1,     1,     1],\n",
       "         [ 7924, 16910,  3606,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [19338, 25354, 18637, 11235,    12,    12,    12, 31079, 19490, 13932],\n",
       "         [ 8996, 31005, 13223, 24482, 16124, 30262,     1,     1,     1,     1],\n",
       "         [19737,  3414,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0, 14165, 19428,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 1262,  2529,     7, 19078,     9,     1,     1,     1,     1,     1],\n",
       "         [16910, 11064,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [25067, 16843,  9601,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [ 1689, 12821,  4739, 21792,   794, 17198,  6878,    12,    12,    12],\n",
       "         [16420, 19347, 27636, 20135,     1,     1,     1,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([[ 9176, 22774,  6487,  ...,  8056,  1927, 16840],\n",
       "         [19920,  5663, 12685,  ...,  1557, 26825, 16743],\n",
       "         [15099,  7949,   794,  ..., 16910,    15, 13932],\n",
       "         ...,\n",
       "         [25067, 16843,  9601,  ..., 21991, 14033, 12569],\n",
       "         [11042, 12821, 19739,  ..., 26511,  6064, 25403],\n",
       "         [27696, 10190, 19683,  ..., 21494, 26818,  1507]], device='cuda:0',\n",
       "        dtype=torch.int32),\n",
       " 'categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'target': tensor([ 9.9035, 10.3785, 10.2577, 10.1628, 10.9151,  9.8256, 10.5321,  9.7410,\n",
       "         10.4487, 11.3096, 10.2220, 10.4487, 10.1849, 10.0213,  9.9988, 10.2341,\n",
       "         11.1199,  9.6396, 10.3890,  9.9759, 10.3482,  9.2302, 10.8198,  9.5469,\n",
       "         10.3890,  9.8522, 10.7144, 10.3890, 10.6573, 10.0213, 10.0213, 10.4268,\n",
       "          9.4472, 11.4076, 10.7790,  9.3623, 10.4631, 10.4043,  9.8522,  9.9270,\n",
       "          9.7112, 10.3890, 10.0213,  9.7982, 11.0021,  9.6159, 10.8198, 10.2810,\n",
       "          9.7700, 10.7144,  9.7982, 10.0450, 10.7144, 10.1267, 10.3890,  9.6396,\n",
       "         10.1437, 10.7144, 10.1024,  9.7002, 10.7364, 10.5133, 10.6573, 10.7171],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Base Architecture\n",
    "\n",
    "Our basic model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch doesn't have implementation for the Global Max Pooling, so we'll implement it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Title Encoder as shown on the picture above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens, pad_ix, hid_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for titles.\n",
    "        x -> emb -> conv -> relu -> global_max\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.emb = nn.Embedding(n_tokens, 32, padding_idx=pad_ix)\n",
    "        self.conv1 = nn.Conv1d(32, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = self.emb(batch)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        c = self.conv1(x)\n",
    "        p = self.pool1(self.relu(c))\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = BaseTitleEncoder(n_tokens=len(preprocessor.vocab), pad_ix=preprocessor.PAD_IX, hid_size=64)\n",
    "title_encoder.to(DEVICE)\n",
    "\n",
    "dummy_x = Variable(batch[\"title_text\"])\n",
    "dummy_v = title_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title_text': tensor([[24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'description_text': tensor([[24816, 26825, 30259, 29588,   819, 24816, 26825, 30259, 14759, 29588,\n",
       "          11235,   819, 23396, 24881, 13932,     0,    14, 25722,    14, 11235,\n",
       "          23397, 30147, 15941,  8200,    14, 20844,  8286,    14,  1106,  3973,\n",
       "          14235,  1927,     0, 21736,    15, 27600, 13699,  4640, 30041, 12710,\n",
       "           9263,  1883, 19240,    12,    12,    12,    16,    12,    12,    12,\n",
       "          30147,  2591, 15954,   819, 31005, 10309, 30158, 27696, 10848,    14,\n",
       "          21486,  2591,   819, 24816, 16073,    15, 27600, 24816, 26825, 30259,\n",
       "          30041, 12710, 27916,  3220,   898, 27916,  6152,  1927, 13886,  4739,\n",
       "          20988,    14,  2389, 29809,  2389,  2328, 14063,  7402, 19941,  1927,\n",
       "           9527, 23397, 27916, 27125,  1883,  1140, 23897, 13932,  1669,  2467],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591, 22873,   794,    12,\n",
       "             12,    12,  3131,  5353, 10158, 23174,  1927,  3370, 19920,  5663,\n",
       "          14759, 24683,   819, 27144,     0, 17204,    16, 26233, 17979, 27916,\n",
       "          20275, 13932, 27600,  8058, 19403,  7301,  9139, 17982, 11235,  2126,\n",
       "          13932, 27600, 20771,    14,  1513,  1927,  5335, 14078,    15, 27600,\n",
       "          21236, 30041,  9670, 16859, 25235,  6588, 27916, 24941, 21872, 13932,\n",
       "          25302, 17981,  1927, 13013,  6532, 24537,    15, 27600, 21657, 23337,\n",
       "           2273,   819, 26511,  2927, 13932, 26237, 19745, 17208,    14,   819,\n",
       "           7977, 27916, 30246, 13932,   819, 20771,    16,  3568, 17981,  9748,\n",
       "             14,  1178,  1927,   819, 12115, 27290, 10001,    15, 15099, 22148]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " 'categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " 'target': tensor([ 9.7115, 10.4631], device='cuda:0')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = training_data[:2]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24816, 26825, 30259,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [26233,    16, 17204, 17979, 13932, 20771,  3591,     1,     1,     1]],\n",
       "        device='cuda:0', dtype=torch.int32),\n",
       " tensor([[[-0.7320,  0.9132, -0.4246,  0.8031,  1.0000,  1.8740, -0.8099,\n",
       "           -0.2754, -0.6593,  0.3797],\n",
       "          [-0.5913,  1.2903, -0.0850, -0.0205, -0.0640, -0.8971,  2.0486,\n",
       "           -1.2139,  1.2958, -0.7082],\n",
       "          [-0.0417, -1.2509, -0.2800,  0.0814,  0.4712,  0.0078,  0.3499,\n",
       "           -0.0745, -0.5263, -0.9447],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.7441,  1.7484, -0.1476, -1.1068, -0.0635,  1.6631,  1.2102,\n",
       "           -0.5831,  0.5467, -0.9735],\n",
       "          [ 1.4190, -0.4109,  0.1628,  1.5385, -1.1865, -0.1865, -0.4308,\n",
       "           -2.1818, -0.5515,  0.2146],\n",
       "          [ 0.8362, -0.0774,  0.0764,  0.3787, -0.6565,  0.8575, -0.4299,\n",
       "           -2.0215, -0.7235, -0.0157],\n",
       "          [ 0.5842, -1.5506, -0.3510,  0.0226,  1.3138, -1.0777, -0.7317,\n",
       "           -0.3829, -0.5747,  0.3410],\n",
       "          [-1.5054,  0.7809,  1.6062, -0.5702,  0.8563, -0.0807,  1.2376,\n",
       "           -0.5906, -1.4686,  0.7504],\n",
       "          [ 0.6708,  0.2308, -1.6952,  2.6060, -0.6319, -0.7812,  1.0425,\n",
       "            0.6098, -1.0802, -1.0564],\n",
       "          [ 1.0125,  0.5314, -1.0024,  0.2054,  0.3092, -1.0967, -2.1028,\n",
       "           -0.0537,  0.3467,  1.3176],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000]]], device='cuda:0',\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(\n",
    "    num_embeddings=len(preprocessor.vocab), embedding_dim=10, padding_idx=preprocessor.PAD_IX, device=DEVICE\n",
    ")\n",
    "title, embedded_title = batch[\"title_text\"], emb(batch[\"title_text\"])\n",
    "title, embedded_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(embedded_title, 1, 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Description Encoder as shown on the picture above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescriptionEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens, pad_ix, hid_size=64):\n",
    "        \"\"\"\n",
    "        A simple sequential encoder for descriptions.\n",
    "        x -> emb -> conv -> relu -> global_max\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.emb = nn.Embedding(n_tokens, 32, padding_idx=pad_ix)\n",
    "        self.conv1 = nn.Conv1d(32, hid_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        x = self.emb(batch)\n",
    "\n",
    "        # we transpose from [batch, time, units] to [batch, units, time] to fit Conv1d dim order\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        c = self.conv1(x)\n",
    "        p = self.pool1(self.relu(c))\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "description_encoder = BaseDescriptionEncoder(n_tokens=len(preprocessor.vocab), pad_ix=preprocessor.PAD_IX, hid_size=64)\n",
    "description_encoder.to(DEVICE)\n",
    "\n",
    "dummy_x = Variable(batch[\"description_text\"])\n",
    "dummy_v = description_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del description_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusion: Title Encoder + Description Encoder + Categorical Encoder + FC Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_cat_features, n_tokens, pad_ix, hid_size=64):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.title_encoder = BaseTitleEncoder(n_tokens=n_tokens, pad_ix=pad_ix, hid_size=64)\n",
    "        self.desc_encoder = BaseDescriptionEncoder(n_tokens=n_tokens, pad_ix=pad_ix, hid_size=64)\n",
    "\n",
    "        self.cat_dense1 = nn.Linear(n_cat_features, hid_size)\n",
    "        self.cat_dense2 = nn.Linear(hid_size, hid_size)\n",
    "\n",
    "        self.out_dense1 = nn.Linear(192, 100)\n",
    "        self.out_dense2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # process each data source with it's respective encoder\n",
    "        title_h = self.title_encoder(batch[\"title_text\"])\n",
    "        desc_h = self.desc_encoder(batch[\"description_text\"])\n",
    "\n",
    "        # apply categorical encoder\n",
    "        cat_h = self.relu(self.cat_dense1(batch[\"categorical\"]))\n",
    "        cat_h = self.cat_dense2(cat_h)\n",
    "\n",
    "        # concatenate all vectors together...\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "\n",
    "        # ... and stack a few more layers at the top\n",
    "        joint_h = self.out_dense1(joint_h)\n",
    "        joint_h = self.relu(joint_h)\n",
    "        joint_h = self.out_dense2(joint_h)\n",
    "\n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "\n",
    "        return joint_h[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseSalaryPredictor(\n",
    "    n_cat_features=len(preprocessor.categorical_vectorizer.vocabulary_),\n",
    "    n_tokens=len(preprocessor.vocab),\n",
    "    pad_ix=preprocessor.PAD_IX,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "\n",
    "dummy_pred = model(batch)\n",
    "dummy_loss = criterion(dummy_pred, batch[\"target\"])\n",
    "assert dummy_pred.shape == torch.Size([64])\n",
    "assert len(torch.unique(dummy_pred)) > 20, \"model returns suspiciously few unique outputs. Check your initialization\"\n",
    "assert dummy_loss.ndim == 0 and 0.0 <= dummy_loss <= 250.0, \"make sure you minimize MAE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    iter_size = 1000\n",
    "    for batch_num, batch in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        y = batch[\"target\"]\n",
    "        pred = model(batch)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % iter_size == 0:\n",
    "            loss, current = loss.item(), batch_num * len(y)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            y = batch[\"target\"]\n",
    "            pred = model(batch)\n",
    "\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            squared_error += torch.sum(torch.square(pred - y))\n",
    "            abs_error += torch.sum(torch.abs(pred - y))\n",
    "            num_samples += len(pred)\n",
    "\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    print(f\"Test Error: \\n MSE: {mse:>0.2f}, MAE: {mae:>0.2f}, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 334.780151  [    0/195814]\n",
      "loss: 9.446263  [32000/195814]\n",
      "loss: 11.764561  [64000/195814]\n",
      "loss: 11.522502  [96000/195814]\n",
      "loss: 8.767136  [128000/195814]\n",
      "loss: 9.265714  [160000/195814]\n",
      "loss: 9.236448  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.12, MAE: 0.26, Avg loss: 8.445479 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 8.999011  [    0/195814]\n",
      "loss: 11.923000  [32000/195814]\n",
      "loss: 9.187837  [64000/195814]\n",
      "loss: 8.116298  [96000/195814]\n",
      "loss: 7.326610  [128000/195814]\n",
      "loss: 7.400521  [160000/195814]\n",
      "loss: 6.846798  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.10, MAE: 0.24, Avg loss: 7.628371 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 7.218437  [    0/195814]\n",
      "loss: 9.125703  [32000/195814]\n",
      "loss: 6.340958  [64000/195814]\n",
      "loss: 9.970930  [96000/195814]\n",
      "loss: 6.718946  [128000/195814]\n",
      "loss: 7.820122  [160000/195814]\n",
      "loss: 4.916142  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.10, MAE: 0.23, Avg loss: 7.342856 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 5.972039  [    0/195814]\n",
      "loss: 6.450713  [32000/195814]\n",
      "loss: 8.181159  [64000/195814]\n",
      "loss: 7.502969  [96000/195814]\n",
      "loss: 6.049113  [128000/195814]\n",
      "loss: 4.969719  [160000/195814]\n",
      "loss: 6.040085  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.22, Avg loss: 7.042449 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 6.197118  [    0/195814]\n",
      "loss: 8.233703  [32000/195814]\n",
      "loss: 5.128002  [64000/195814]\n",
      "loss: 6.240980  [96000/195814]\n",
      "loss: 5.823710  [128000/195814]\n",
      "loss: 6.111752  [160000/195814]\n",
      "loss: 6.673345  [192000/195814]\n",
      "Test Error: \n",
      " MSE: 0.09, MAE: 0.22, Avg loss: 7.003364 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = BaseSalaryPredictor(\n",
    "    n_cat_features=len(preprocessor.categorical_vectorizer.vocabulary_),\n",
    "    n_tokens=len(preprocessor.vocab),\n",
    "    pad_ix=preprocessor.PAD_IX,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.L1Loss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yandex_nlp",
   "language": "python",
   "name": "yandex_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
